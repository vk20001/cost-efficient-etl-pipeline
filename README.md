# ğŸ› ï¸ Cost-Efficient ETL Pipeline with CI/CD, Validation & Monitoring

A production-style ETL pipeline that ingests, cleans, validates, and loads **Brazil E-Commerce** data using **Python, Airflow, PostgreSQL, Great Expectations, Docker, and GitHub Actions** â€” all without relying on paid tools or bloated stacks.

## ğŸ“¦ Project Stack

| Layer         | Tech Used                            |
|---------------|--------------------------------------|
| Orchestration | Apache Airflow (via Docker)          |
| Processing    | Python (Pandas, SQLAlchemy)          |
| Validation    | Great Expectations                   |
| Storage       | PostgreSQL (Dockerized)              |
| Monitoring    | Logging + Airflow DAG alerting       |
| CI/CD         | GitHub Actions + Pytest              |
| Config Mgmt   | YAML (`config/config.yml`) for dataset â†” table mapping |

## ğŸ“ˆ Key Features

âœ… **ETL Pipeline**  
- Modular and reusable cleaners for each CSV dataset  
- Loads data into PostgreSQL using SQLAlchemy  
- Config-driven with `.env` + YAML support  
- Logs row counts and execution time  

âœ… **Data Validation with Great Expectations**  
- Expectations configured per dataset  
- Validated automatically via Airflow  
- Dynamic Postgres datasource setup  
- Fails loudly if expectations arenâ€™t met  

âœ… **Monitoring (No Grafana Needed)**  
- ETL writes structured logs (`logs/etl.log`)  
- Airflow DAG scans logs daily for failures or zero rows  
- Optional Grafana/Prometheus config available in `monitoring/`  

âœ… **CI/CD with GitHub Actions**  
- Runs `pytest` for unit tests  
- Executes Great Expectations validations  
- Easy to extend for deployment or DAG sync  

## ğŸ§ª Sample Unit Test

```python
def test_customers_schema():
    df = pd.read_csv("data/raw/olist_customers_dataset.csv")
    df_clean = clean_customers(df)
    assert "customer_id" in df_clean.columns
    assert not df_clean.isnull().any().any()
```

> Tests for schema, nulls, row counts, and cleaner coverage are all included in `tests/`

## ğŸ³ Docker Customisations

- `docker/dockerfile.airflow` extends the base Airflow image to include:
  - `dbt-core==1.7.9`
  - `dbt-postgres==1.7.9`
- Keeps orchestration, transformation, and validation unified inside the Airflow container

## ğŸ“ Folder Structure

```
.
â”œâ”€â”€ config/                      â† Global YAML configs + dbt profiles
â”‚   â””â”€â”€ config.yml
â”œâ”€â”€ dags/                        â† Airflow DAGs
â”œâ”€â”€ data/                        â† Raw CSVs (not committed)
â”œâ”€â”€ dbt_project/                 â† dbt models & project files
â”œâ”€â”€ docker/                      â† Custom Dockerfiles
â”œâ”€â”€ gx/                          â† Great Expectations suite
â”œâ”€â”€ logs/                        â† ETL & validation logs
â”œâ”€â”€ monitoring/                  â† (Optional) Prometheus & Grafana configs
â”œâ”€â”€ src/                         â† Pipeline code
â”‚   â””â”€â”€ etl/                     â† ETL runners & cleaners
â”‚   â””â”€â”€ validation/              â† GE checkpoint runner
â”œâ”€â”€ tests/                       â† Pytest unit tests
â”œâ”€â”€ .github/workflows/ci.yml     â† CI pipeline
â”œâ”€â”€ docker-compose.yml           â† Dev stack launcher
â””â”€â”€ requirements.txt
```

## ğŸ“Š Data Source

This project uses the [Brazilian E-Commerce Public Dataset](https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce), which includes customer, order, product, seller, and geolocation data.

## ğŸš€ How to Run Locally

```bash
# Clone the repo
git clone https://github.com/your-username/cost-efficient-etl-pipeline.git
cd cost-efficient-etl-pipeline

# Launch all services
docker-compose up --build

# Open Airflow at: http://localhost:8080
```

## ğŸ§ª Run Tests Locally

```bash
# In your local environment (not container)
pip install -r requirements.txt
pytest tests/
```

## âœ… CI/CD via GitHub Actions

The [`.github/workflows/ci.yml`](.github/workflows/ci.yml) file automatically runs on every `push` and `pull_request` to `main`:

- âœ… **Code Quality** â€” Linting with [`ruff`](https://github.com/astral-sh/ruff), a fast Python linter  
- âœ… **Unit Tests** â€” Runs `pytest` with coverage on the ETL pipeline  
- âœ… **Validation** â€” Executes Great Expectations checkpoints on actual datasets  
- âœ… **Isolated Environment** â€” Uses a service container for PostgreSQL  
- âœ… **Fail Fast** â€” Pipeline fails early on data/schema/test issues  





