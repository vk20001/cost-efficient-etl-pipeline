global:
  scrape_interval: 15s

scrape_configs:
  - job_name: "etl_pipeline"
    static_configs:
      # Prometheus is in its own container, Airflow tasks run in the
      # webserver/scheduler containers.  Each run spins up a short-lived
      # Python process that exposes :8000 while itâ€™s alive.
      - targets:   ["172.31.35.118:8000"]
